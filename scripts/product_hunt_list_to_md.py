import os
import requests
from datetime import datetime, timedelta, timezone
from openai import OpenAI
from bs4 import BeautifulSoup
import pytz

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

producthunt_client_id = os.getenv('PRODUCTHUNT_CLIENT_ID')
producthunt_client_secret = os.getenv('PRODUCTHUNT_CLIENT_SECRET')

class Product:
    def __init__(self, id: str, name: str, tagline: str, description: str, votesCount: int, createdAt: str, featuredAt: str, website: str, url: str, **kwargs):
        self.name = name
        self.tagline = tagline
        self.description = description
        self.votes_count = votesCount
        self.created_at = self.convert_to_seoul_time(createdAt)
        self.featured = "ì˜ˆ" if featuredAt else "ì•„ë‹ˆì˜¤"
        self.website = website
        self.url = url
        self.og_image_url = self.fetch_og_image_url()
        self.keyword = self.generate_keywords()
        self.translated_tagline = self.translate_text(self.tagline)
        self.translated_description = self.translate_text(self.description)

    def fetch_og_image_url(self) -> str:
        """ì œí’ˆì˜ Open Graph ì´ë¯¸ì§€ URL ê°€ì ¸ì˜¤ê¸°"""
        response = requests.get(self.url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            og_image = soup.find("meta", property="og:image")
            if og_image:
                return og_image["content"]
        return ""

    def generate_keywords(self) -> str:
        """ì œí’ˆì˜ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì—¬ í•œ ì¤„ë¡œ í‘œì‹œ (ì˜ì–´ ì‰¼í‘œë¡œ êµ¬ë¶„)"""
        prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”. í‚¤ì›Œë“œëŠ” ì˜ì–´ ì‰¼í‘œë¡œ êµ¬ë¶„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤:\n\nì œí’ˆ ì´ë¦„: {self.name}\n\nìŠ¬ë¡œê±´: {self.tagline}\n\nì„¤ëª…: {self.description}"
        
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ì œí’ˆ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì ì ˆí•œ í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”. í‚¤ì›Œë“œëŠ” ì˜ì–´ ì‰¼í‘œë¡œ êµ¬ë¶„ë˜ì–´ í•œ ì¤„ì— í‘œì‹œí•´ì•¼ í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=50,
                temperature=0.7,
            )
            keywords = response.choices[0].message.content.strip()
            if ',' not in keywords:
                keywords = ', '.join(keywords.split())
            return keywords
        except Exception as e:
            print(f"í‚¤ì›Œë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return "í‚¤ì›Œë“œ ì—†ìŒ"

    def translate_text(self, text: str) -> str:
        """OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë²ˆì—­í•©ë‹ˆë‹¤."""
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì„¸ê³„ ìµœê³ ì˜ ë²ˆì—­ ë„êµ¬ì…ë‹ˆë‹¤. ì˜ì–´ì™€ í•œêµ­ì–´ë¥¼ ëŠ¥ìˆ™í•˜ê²Œ ë²ˆì—­í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. IT ì „ë¬¸ ìš©ì–´ë‚˜ í‘œí˜„ì„ ì‰½ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ ì£¼ì„¸ìš”. ì•„ë˜ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”."},
                    {"role": "user", "content": text},
                ],
                max_tokens=500,
                temperature=0.7,
            )
            translated_text = response.choices[0].message.content.strip()
            return translated_text
        except Exception as e:
            print(f"ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return text

    def convert_to_seoul_time(self, utc_time_str: str) -> str:
        """UTC ì‹œê°„ì„ ì„œìš¸ ì‹œê°„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        utc_time = datetime.strptime(utc_time_str, '%Y-%m-%dT%H:%M:%SZ')
        seoul_tz = pytz.timezone('Asia/Seoul')
        seoul_time = utc_time.replace(tzinfo=pytz.utc).astimezone(seoul_tz)
        return seoul_time.strftime('%Yë…„%mì›”%dì¼ %p%I:%M (ì„œìš¸ ì‹œê°„)')

    def to_markdown(self, rank: int) -> str:
        """ì œí’ˆ ë°ì´í„°ë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
        og_image_markdown = f"![{self.name}]({self.og_image_url})"
        return (
            f"## [{rank}. {self.name}]({self.url})\n\n"
            f"**ìŠ¬ë¡œê±´:**\n{self.translated_tagline}\n\n"
            f"**ì†Œê°œ:**\n{self.translated_description}\n\n"
            f"**ì œí’ˆ ì›¹ì‚¬ì´íŠ¸:**\n[ë°”ë¡œ ë°©ë¬¸]({self.website})\n\n"
            f"**í”„ë¡œë•íŠ¸ í—ŒíŠ¸:**\n[View on Product Hunt]({self.url})\n\n"
            f"{og_image_markdown}\n\n"
            f"**í‚¤ì›Œë“œ:** {self.keyword}\n\n"
            f"**íˆ¬í‘œìˆ˜:** ğŸ”º{self.votes_count}\n\n"
            f"**í”¼ì²˜ ì—¬ë¶€:** {self.featured}\n\n"
            f"**ë°œí‘œ ì‹œê°„:** {self.created_at}\n\n"
            f"---\n\n"
        )

def get_producthunt_token():
    """client_idì™€ client_secretì„ ì‚¬ìš©í•˜ì—¬ Product Huntì˜ access_tokenì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    url = "https://api.producthunt.com/v2/oauth/token"
    payload = {
        "client_id": producthunt_client_id,
        "client_secret": producthunt_client_secret,
        "grant_type": "client_credentials",
    }

    headers = {
        "Content-Type": "application/json",
    }

    response = requests.post(url, json=payload, headers=headers)

    if response.status_code != 200:
        raise Exception(f"access tokenì„ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {response.status_code}, {response.text}")

    token = response.json().get("access_token")
    return token

def fetch_product_hunt_data():
    """Product Huntì—ì„œ ì „ë‚ ì˜ Top 30 ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    token = get_producthunt_token()
    yesterday = datetime.now(timezone.utc) - timedelta(days=1)
    date_str = yesterday.strftime('%Y-%m-%d')
    url = "https://api.producthunt.com/v2/api/graphql"
    headers = {"Authorization": f"Bearer {token}"}

    base_query = """
    {
      posts(order: VOTES, postedAfter: "%sT00:00:00Z", postedBefore: "%sT23:59:59Z", after: "%s") {
        nodes {
          id
          name
          tagline
          description
          votesCount
          createdAt
          featuredAt
          website
          url
        }
        pageInfo {
          hasNextPage
          endCursor
        }
      }
    }
    """

    all_posts = []
    has_next_page = True
    cursor = ""

    while has_next_page and len(all_posts) < 30:
        query = base_query % (date_str, date_str, cursor)
        response = requests.post(url, headers=headers, json={"query": query})

        if response.status_code != 200:
            raise Exception(f"Product Hunt ë°ì´í„° ê°€ì ¸ì˜¤ê¸°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {response.status_code}, {response.text}")

        data = response.json()['data']['posts']
        posts = data['nodes']
        all_posts.extend(posts)

        has_next_page = data['pageInfo']['hasNextPage']
        cursor = data['pageInfo']['endCursor']

    # ìƒìœ„ 30ê°œì˜ ì œí’ˆë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    return [Product(**post) for post in sorted(all_posts, key=lambda x: x['votesCount'], reverse=True)[:30]]

def generate_markdown(products, date_str):
    """Markdown ë‚´ìš©ì„ ìƒì„±í•˜ì—¬ data ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤."""
    markdown_content = f"# PH ì˜¤ëŠ˜ì˜ ì¸ê¸° ì œí’ˆ | {date_str}\n\n"
    for rank, product in enumerate(products, 1):
        markdown_content += product.to_markdown(rank)

    # data ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.
    os.makedirs('data', exist_ok=True)

    # data ë””ë ‰í† ë¦¬ì— íŒŒì¼ ì €ì¥
    file_name = f"data/PH-daily-{date_str}.md"
    
    # íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë®ì–´ì”ë‹ˆë‹¤.
    with open(file_name, 'w', encoding='utf-8') as file:
        file.write(markdown_content)
    print(f"íŒŒì¼ {file_name} ìƒì„± ë° ë®ì–´ì“°ê¸° ì™„ë£Œ.")

def main():
    # ì–´ì œ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸° ë° í¬ë§·íŒ…
    yesterday = datetime.now(timezone.utc) - timedelta(days=1)
    date_str = yesterday.strftime('%Y-%m-%d')

    # Product Hunt ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    products = fetch_product_hunt_data()

    # Markdown íŒŒì¼ ìƒì„±
    generate_markdown(products, date_str)

if __name__ == "__main__":
    main()
